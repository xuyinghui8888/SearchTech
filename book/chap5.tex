
\chapter{实时计算背后的核心技术}
\thispagestyle{empty}

\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
\noindent\shadowbox{
\begin{tcolorbox}[arc=0mm,colback=lightblue,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~强化学习．} \\ 

\end{tcolorbox}}
\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt} 

\section{实时计算的意义}
相比传统的网页搜索引擎，淘宝搜索由于以下几个原因对实时性的要求非常高：

首先，众所周知，淘宝具有很强的动态性，宝贝的循环搁置，新卖家加入，卖家新商品的推出，价格的调整，标题的更新，旧商品的下架，换季商品的促销，宝贝图片的更新，销量的变化，卖家等级的提升，等等，都需要搜索引擎在第一时间捕捉到这些变化，并在最终的排序环节，把这些变化及时地融入匹配和排序，带来结果的动态调整。

其次，从2013年起，淘宝搜索就进入千人千面的个性化时代，搜索框背后的查询逻辑，已经从基于原始Query演变为【Query+用户上下文+地域+时间】，搜索不仅仅是一个简单根据输入而返回内容的不聪明的“机器”，而是一个能够自动理解、甚至提前猜测用户意图（比如用户浏览了一些女士牛仔裤商品，然后进入搜索输入查询词“衬衫”，系统分析用户当前的意图是找女性相关的商品，所以会展现更多的女士衬衫，而不是男生衬衫），并能将这种意图准确地体现在返回结果中的聪明系统，这个系统在面对不同的用户输入相同的查询词时，能够根据用户的差异，展现用户最希望看到的结果。变化是时刻发生的，商品在变化，用户个体在变化，群体、环境在变化。在搜索的个性化体系中合理地捕捉变化，正是实时个性化要去解决的课题。

最后，电商平台也完成了从PC时代到移动时代的转变，随着移动时代的到来，人机交互的便捷、碎片化使用的普遍性、业务切换的串行化，要求我们的系统能够对变化莫测的用户行为以及瞬息万变的外部环境进行完整的建模。

\section{在线学习系统架构}
早先的搜索学习能力，主要是基于批处理的离线机器学习。每天离线训练模型，然后将训练得到的模型推送到线上系统做打分预测，线上预测阶段模型不会变化。这种离线模型+在线预测也是工业界常用的模式，但如上节说过，淘宝搜索因为其自身的特征，这种模式有很大弊端。在离线批量学习中，一般会假设样本独立服从一个未知的分布，但如果分布变化，模型效果会明显降低。而在淘宝搜索业务中，很多情况下，一个模型生效后，样本的分布会发生大幅变化，因此离线学到的模型已经不能很好地匹配线上数据了。所以需要通过在线学习，不断地实时调整模型，拟合最新的线上数据，特别是在大促这种数据分布变化很大的场景下。

为了满足在线学习的需求，搜索技术团队自主开发了基于parameter server的在线学习框架，如下图所示，目前已经实现了FTRL，矩阵分解，深度学习等多种在线学习模型，模型更新后会实时推送到线上打分引擎。

	
\section{在线FTRL}
逻辑斯蒂回归（LogisticRegrssion）应该是工业界最经常使用的一个模型，能处理超大规模的样本量和特征量。为了防止学习算法过拟合（overfitting），通常会在算法中加入正则项（regularization）。在逻辑斯蒂回归模型中，优化目标一般如下式：

上式中的第一项是模型loss，第二项为正则项。最常见的正则是使用L2-norm，这是由于L2-norm是smooth and strong convex，导数比较好计算。另外一种常见的是L1-norm，相对L2-norm，L1-norm产生的解容易落在坐标轴上，落在坐标轴也就意味着对应的w为0，从而得到稀疏解。L1-norm的好处在于可以使模型得到更多的稀疏解。稀疏解不仅能提升模型的泛化能力，而且在线部署阶段也能节省模型存储空间。

但传统的在线优化算法，比如随机梯度下降（SGD），并不能有效得到稀疏解。如何onlinelearning得到稀疏解也是一个研究的课题。

\paragraph{Truncated Gradient}
为了得到稀疏解，最简单粗暴的方式就是设定一个阈值，当权重小于这个阈值时将其设置为0，如下面的伪代码。这种方法实现起来很简单，也容易理解。但实际中应用中，简单进行截断会造成这部分特征的丢失。
上述的截断法太过简单粗暴，因此TG在此基础上进行了改进，如下：
其中𝜆(𝑡)∈ℝ且𝜆(𝑡)≥0。TG同样是以𝑘为窗口，每𝑘步进行一次截断。当𝑡/𝑘不为整数时𝜆(𝑡)=0，当𝑡/𝑘为整数时𝜆(𝑡)=𝑘𝜆。从公式(3-1-3)可以看出，𝜆和𝜃决定了𝑊的稀疏程度，这两个值越大，则稀疏性越强。尤其令𝜆=𝜃时，只需要通过调节一个参数就能控制稀疏性。

\paragraph{FOBOS}
先前向后切分（FOBOS，Forward-Backward Splitting）是由John Duchi 和Yoram Singer提出，在FOBOS中，将权重的更新分为两个步骤：
前一个步骤就是一个标准的梯度下降过程，后一步是对前一步下降的结果进行微调。观察第二个步骤发现对W的微调也分两部分，第一部分保证微调不要偏离第一步找到的梯度下降方向太远，第二部分则用于处理正则化，产生稀疏解。

\paragraph{RDA}
前面提到的TG、FOBOS都还是建立在SGD的基础上，属于梯度下降类型的方法，这类型方法的优点就是精度比较高，并且TG和FOBOS也都能产生稀疏解。正则对偶平均（RDA，Regularized Dual Average）是从另一个方面来求解Online Optimization。RDA算法中优化的是前T轮所有loss的平均，可以达到平均效果较好。

\paragraph{FTRL: Follow The Regularized Leader}
FTRL是google McMahan提出的算法，它综合考虑了FOBOS和RDA的优点，兼具FOBOS的精确性和RDA优良的稀疏性，其特征权重更新公式为：

该优化公式分为3部分，第一部分表示累计梯度和，就是让损失函数下降的方向；第二部分是表示新的迭代结果不要偏离已经产生的迭代结果太远；第三部分是正则项，有用于产生低遗憾的强凸二范数和产生稀疏解的一范数。另外值得一提的是FTRL采用针对每个特征不同的自适配的学习速率，直观的解释是如果特征出现的次数多，对这个这个特征的权重比较置信，学习速率就比较小，对于很少出现的特征学习速率就比较大。

\section{在线FTRL stacking on GBDT}
	@元涵

\section{在线矩阵分解}
	@达卿，@席奈

\section{在线深度学习，WDL模型} 
	@京五 @元涵

\section{Online Learning To Rank} 
本章主要讲解如何利用Online Learning To Rank的思想对之前产出的各种模型分数进行ensemble。

在之前的叙述中，无论是FTRL分或者个性化分或者商品网络效应分等等都会在一次搜索中给一个宝贝打上不同维度的特征分$s_i\in R$。如何对这些特征分做合理的ensemble是产出排序之前最后的一步。传统的方法，每当有一个新分数$s_{new}$开发完成后，计算新分数$s_{final} = s_{raw} + \gamma s_{new}$会人工调整新加特征分数的权重$\gamma$，通过A/B测试，选取该特征分合理的权重。
其实，不同的特征分之间由于训练目标大多是ctr、cvr这样的指标，不可避免的有着一些耦合性，因此我们期望采取learning的方式自动地选出最合适的ensemble方案，以进一步优化线上指标。

基于可解释性及性能等原因考虑，我们定义最终ensemble形式为线性形式如下：
$$s_{final} = f(s) = w^Ts; s\in R^m; w\in R^m, w_i>0$$
其中$s_i$在前期产出时已经保证是大于0的实数，$w$是我们需要学习的模型参数

\paragraph{Online Learning及意义}
为了学习出合适的$w$，我们完全可以采用离线训练的方式，抽取前n天曝光而没有行为的宝贝作为负例，在搜索场景有点击、购买的宝贝作为正例，训练带权的LR( logistic regression) 来形成一个point wise的ltr任务。

当商品得到足够多的曝光，且搜集的数据充分，label准确的情况下，确实可行；但对于淘宝搜索这个商品量巨大，且label只能通过隐式反馈来提取的场景，这就不太合适了。

以下举个例子来说明训练的模型和采集的样本分布有很大的关系：假设有$s_1, s_2$2个特征分，首先我们完全依据$s_1$进行排序，top的商品都有相同的较大的$s_1$；基于这样的样本学习在$s_1$上缺乏区分度，因此学习到的模型更倾向于给$s_2$一个大的权重。然后我们再完全依据$s_2$进行排序，情况相反，模型会更倾向于给$s_1$一个大的权重。

由于每次新生效的ensemble权重模型会影响线上排序结果即曝光的商品，而转而影响训练结果。
如此循环的情况下，我们采取Online Learning的思路，能在秒级完成模型更新，分钟级进行模型投放，尽可能通过频繁更换投放模型来减小样本分布导致的训练问题。

此外，双十一那样的大促场景，用户的购买习惯也不是前期的模型所能包含的，实时搜集结果、训练、投放的循环，也更为合理。

\paragraph{Learning to Rank算法应用}
在算法部分，我们已经尝试过pointwise、pairwise、listwise的训练方法，下面我们以pointwise为例说一下大致的实现过程。
对于pointwise类的，我们修改我们的ensemble方式为$s_{final} = f(s) = w(c)^Ts$，其中$c\in R^k$表示了用户的一些contextual信息。$w(c)$的函数形式可以是一个神经网络或者其他可导的形式。

类似于LR的思路，定义loss：
$$L(\textbf{w}) = \sum_{j} n_jlog(1+exp(-y_j\textbf{w}(c)^T\textbf{s}_j))$$
其中$\textbf{s}_j\in{R}^m$是每个排序中商品j用不同方法产生的待ensemble的分数，$\textbf{w}(c)\in{R}^m$是所学的特征分的权重，
$y_j\in\{-1,+1\}$是正负样本的label，$n_j\in{R}$是不同样本根据来源例如点击、成交不同而带的不同权重。

基于Porsche平台，我们可以秒级地处理淘宝搜索的最新日志。
当某个用户点击、加购或者购买行为发生后，我们可以关联获取得到用户之前所看过的商品情况，实时地进行特征与label的抽取。
通过类似于SGD或者Rmsprop等基于梯度下降的模型训练方式，对loss进行online的mini batch形式的优化。
每隔一定时间，就进行线上投放。

其他的pairwise及listwise也可以基于此思路进行样本的抽取及算法设计，此处就不再赘述了。但由于online优化支持算法的限制，一些基于树的模型可能无法适用。

\paragraph{多目标实现}
在上节中，我们提到了$n_j\in{R}$所代表的样本权重。实际应用中，通过$n_j\in{R}$不同的设置方式，能够得到不同的排序效果。

\subparagraph{CTR}
在以点击率为目标的时候，不使用成交样本，仅使用点击样本，对所有正负例设置$n_j=1$可以优化点击目标。

\subparagraph{GMV}
在以搜索总成交额为目标的时候，对成交正样本设置$n_j=\lambda Price_j+gap$，对点击正样本及所有负样本设置$n_j=1$可以使优化目标更偏向于成交额
其中$Price_j$为j商品的成交单价。

\subparagraph{GMV+客单价}
在以提升客单价基础上尽可能提升总成交额为目标的时候，对成交正样本设置$n_j=\lambda Price_jG(PriceRank_j)+gap$，对点击正样本及所有负样本设置$n_j=1$。
其中$G(PriceRank_j)$为一个非线性函数，引入G函数的目的是给高价的商品赋予更多的权重，而减少低价商品的权重；$PriceRank_j\in [0,1]$为j商品在叶子类目中的价格排名。
实际G函数形式为$G(PriceRank_j)=exp(\gamma (PriceRank_j-0.5)), \gamma>0$。
通过权重这样的变化，最终在线上排序结果上兼顾客单价及成交额。

以上例举了实际场景中应用较多的目标设置方案。基于pointwise的ltr算法仅通过设置$n_j$就完成了不同目标设置任务。

\paragraph{展望}
完成Online Learning to Rank整个框架之后，相比于传统的手工调整ensemble权重除了效果提升外，还有更加深远的影响。

传统的方法，为了调整b/c商品的比例，我们可能直接通过给某些待ensemble的具体特征分加权来调整。
而如今，借助于Online Ltr技术，仅需要在上层设置一个简单的b/c加权方案来修改$n_j$，就可以调整所有待ensemble的分数的权重，且包含了contextual信息，ensemble手段更加丰富。

目标、手段清晰分离的做法使我们可以更多地把眼光放在目标的设定上，更容易地完成更复杂的任务，例如优化双十一、双十二大促之前的商品加购价值结合GMV的目标等任务。
若配合以实时的监控系统，我们还可以实现更为复杂的闭环调整系统。


\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{\protect\numberline{}{\hspace{-1.5em}参考文献}}
\markboth{参考文献}{参考文献}
\bibitem{1} 搜索双链路实时计算体系@双11实战，http://www.atatech.org/articles/44909
\bibitem{2} 基于在线矩阵分解的淘宝搜索实时个性化, http://www.atatech.org/articles/38646
\bibitem{3} BP如何运行, http://www.offconvex.org/2016/12/20/backprop/
\end{thebibliography}

 
