
\chapter{个性化搜索背后的核心技术}
\thispagestyle{empty}

\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
\noindent\shadowbox{
\begin{tcolorbox}[arc=0mm,colback=lightblue,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~强化学习．} \\ 

\end{tcolorbox}}
\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt} 

个性化搜索最根本的目标是要根据用户的意图和喜好展示搜索结果页，抛弃目前这种千篇一律的展现方式。个性化体现了搜索系统的智能性，从提高用户体验方面加深了用户对淘宝的粘性。

\section{用户肖像建模} 
	物以类聚，人以群分。不同的人群，在总体上有着不同的行为特点和购物需求。我们对用户的了解，是从他/她所属的人群开始的。人群可以按不同的维度划分，如性别、年龄、购买力、地域等。例如，在服饰行业中，男性用户更喜欢买男装，女性用户更喜欢买女装。这样，在用户搜索“T恤”时，我们可以根据他的性别展示更符合他偏好的结果。不同年龄段的用户的购物需求也会有明显的差异，例如穿衣风格或者手机款式。
	为了识别用户所属的人群，需要使用尽量多的数据。最基础的是用户注册的信息，不过这种信息有时并不准确。比如，用户注册时填的不准确，或者用户把账号长期给家人使用。所以还需要使用用户在网站上的行为数据来校正这些数据。这时会使用机器学习的方法，把用户肖像建模看成一个分类问题，使用各种来源的数据来预测用户所属的人群。
	\subsection{性别}
	性别作为用户最重要的基本属性之一，必然是个性化考虑因素。对电子商务网站来讲，性别也是搜索和推荐系统决策因素之一。淘宝主要消费群体是女性，用户数据容易被女性行为主导，人气排序下表现尤为明显。性别个性化则是根据用户的性别影响排序，在用户query没有明确表明性别的情况下提前与用户性别相同的商品，旨在减少翻页次数or换query次数从而提高ctr。另外，如果用户能看到更多与其购买意图相关的商品，可能会提高成交转化率。
	\paragraph{背景}
	为了利用性别影响排序，首先需要解决如何标记商品性别。商品的性别可通过类目或者属性来表现，而类目的性别表现又分为窄义性别和广义性别。窄义性别表现类目有服饰、鞋和包等，此类型只要提前相应类目或者类目下具有某些特征的商品即可；广义性别表现类目包括窄义类目和诸如手机、电脑、游戏币等隐含类别，该类型下商品的性别与类目无关，而是由商品本身的特征决定的，如颜色、风格等（与性别无明显关系），这类性别标签需要挖掘才能发现。
	性别个性化另外一个重要的方面是如何预测用户性别。用户注册时的性别信息和支付宝实名认证都可以作为判断性别的依据，但考虑到用户可能填错以及实名认证用户少、甚至有账号被同时多个用户使用的情况，我们不能直接应用这些信息。个性化用到的性别必须有物理性别与淘宝性别之分，所以必须建立一套合理的性别预测方案。
	\paragraph{建模}
	训练:
	根据一级类目成交的性别占比得到男性、女性、无性别购买类目组（一级类目的子集）。对有购买记录的用户：根据用户在性别相关类目下的ipv及ipv天数、支付宝笔数及金额、虚拟物品笔数占比及不同时间段ipv占比计算各维度权重，得到回归模型。对无购买记录的用户：女性类目总体ipv和ipv天数、男性类目总体ipv及用户注册时长作为预测维度计算权重得到回归模型。
	预测:
	根据用户是否有成交启用不同模型，回归值大于等于0时为女性，否则为男性。
	\paragraph{特征}
	\begin{itemize}
	\item{分别15个女性、男性倾向类目的点击总数}
	\item{分别15个女性、男性倾向类目的购买总数}
	\item{强女性类目点击天数}
	\item{强男性类目点击天数}
	\item{总点击天数}
	\item{强女性类目购买天数}
	\item{强男性类目购买天数}
	\item{总购买天数}
	\item{女性倾向类目点击次数占比}
	\item{男性倾向类目点击次数占比}
	\item{点击占比熵}
	\item{女性倾向类目购买次数占比}
	\item{男性倾向类目购买次数占比}
	\item{购买占比熵}
	\item{强女性、男性类目类目点击天数差占有点击天数的比例}
	\item{强女性、男性类目类目购买天数差占有购买天数的比例}
	\end{itemize}
	\paragraph{效果}
	最终效果，总体召回率：86\%，总体准确率：94\%。
	\begin{table}
		\centering
		\caption{准确率}
		\begin{tabular}
			{|l|l|l|l|l|l|}
			\hline
			新版&样本数&预测男&预测女&召回率&准确率\\
			\hline
			真实男&1380636&1192135&73080&86.3\%&93.4\%\\
			\hline
			真实女&1353568&84840&1156177&85.4\%&94.1\%\\
			\hline
		\end{tabular}
		\label{性别准确率}
	\end{table}
	\subsection{年龄}
	用户年龄的识别可以简单的使用身份证上的生日计算年龄。
	\subsection{购买力}
	随着中国经济和电子商务的快速发展，用户也在快速成长，对于高品质、高端商品的需求不断上升，而目前我们的搜索对于这部分用户需求的满足不是很好，高端用户在不断流失。本项目的旨在通过算法和运营配合挖掘出淘宝上的高端用户，同时展现给这些用户合适的商品，以提高这部分用户的体验和留存率。
	\paragraph{商品的价格档}
	为了便于在业务中分析各种数据，可以将用户的购买力分成几个档次（如1~7档），档次越高表示用户的购买力越大。用户的购物行为中可以很方便的体现购买力。这时需要先确定商品的价格档。由于每个类目商品的价格差异很大，所以需要按类目来划分。
	例如：最近一个月主搜引导成交的笔单价从小到大排序，按指定分位点划分成七档，如：0,0.2,0.4,0.6,0.8,0.9,0.95,1。
	\paragraph{用户购买力模型}
	我们使用了GBDT模型，训练用户的购买力。以未来搜索引导的成交在类目下的价格分档为目标，建立了一个多分类的模型。
	\paragraph{特征}
	\begin{itemize}
		\item{服饰类成交额、笔单价（衣）}
		\item{食品类成交额、笔单价（食）}
		\item{日用品开销（家居百货）}
		\item{是否有房+住房档次（住）}
		\item{装修档次（住）}
		\item{是否有车+车档次（行）}
		\item{酒店门票类开销（行）}
		\item{购买品牌、奢侈品}
		\item{职业}
		\item{教育程度}
		\item{年龄段}
		\item{手机型号}
		\item{资产等级}
		\item{好友关系}
	\end{itemize}
	\subsection{家庭账号}
	但由于家庭账号，或者代买的情况。
	\subsection{用户-商品CTR预估}
	\paragraph{背景}
	“个性化”在淘宝搜索中起着至关重要的作用，即让不同的用户看到最符合自己需求的商品。为了实现这个目标，最直接的方式就是预估商品到不同人群的ctr。当用户搜索时，使用这个分数排序，就可以把符合用户所属人群的商品优先展示。
	\paragraph{建模}
	

\section{匹配学习}
	
	
\subsection{一阶人货匹配模型} 
	@丹鸥 u2i，u2s，u2b 

\subsection{高阶人货匹配模型} 
	@丹鸥 u2i2i，u2s2i，u2b2i 

\subsection{深度匹配模型} 
    @席奈 和序列匹配模型合并。

\subsection{序列匹配模型} 
1，前面三个章节我们递进的描述了用户与单个商品之间的匹配方式和模型。然而，上述方法均假设用户的购物行为之间是独立的——并不存在依赖、相关或序列关系。

举例来说，一个用户$U_1$2天内依次购买了以下商品：烤箱、面粉、奶油；另一个用户$U_2$半年内依次购买了孕妇衣、尿布和奶瓶。我们先考虑用户$U_1$，我们可以从他购买了烤箱和面粉2种商品推断他很可能想要做蛋糕（而这从每个单一买的商品都是很难推断的），因此也许需要奶油；再考虑用户$U_2$，我们可以从她依次购买了孕妇衣和尿布推断她很可能怀孕过并且已经生了小宝宝（从某一件来推荐会比较勉强），因此马上会需要奶瓶等婴儿用品。

从上面例子我们可以看出，用户的购物行为之间往往是存在高阶依赖关系的，即仅用户购买了一个商品集合$\{A, B, C\}$后，才会购买商品$D$；同时，用户的购物行为也会存在序列关系，即用户购买C，仅会在他依次购买了商品A和B之后。在这2种关系下，我们前3节使用的模型会很难捕捉这类规律。因此我们需要一种模型，能整体的考虑用户的行为历史（而不是将其行为拆分成一个一个的单独分析），进而推断他接下来的需求。

下面我们会首先介绍几种经典的序列模型以及带有记忆功能的模型，然后会详细介绍在淘宝搜索中，我们怎样使用这类模型做到用户与商品之间的序列匹配。

2，在机器学习的任务环境中，我们有大量的场景都是需要做一个序列预测和带有记忆的推断的。例如在query自动补全的场景下，我们需要根据用户输入的文字或者词序列来预测用户下一个最可能会输入的词语；又例如有这样一个问题，需要让机器在阅读了一整篇文章后，回答若干关于这个文章的问题。这类问题和场景下都需要模型具有一定的记忆能力，能在获取新信息的同时，记住部分老的信息。

A) 最常用而有效的方式是使用一个递归神经网络模型(RNN \cite{4,5})。正如其名字描述的，递归神经网络在隐层结构上存在一个循环，即当前隐层的输入是上一个隐层的输出以及当前的输入2项一起。由于每个隐层的信息都能递归的输入到下一个隐层中，因此会具有一定的记忆能力。如图\ref{fig:RNN}，我们将RNN按时间序列“打开”，可以看到前一时刻的隐层$S_{t-1}$和当前输入$X_t$会共同影响当前的隐层$S_{t}$。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{"fig/RNN"}
	\caption{递归神经网络(RNN)示意图}
	\label{fig:RNN}
\end{figure}	

然而RNN存在的最大问题是“梯度消失和爆炸”问题\cite{6}。这是因为在神经网络进行反向传播(backpropagation)的时候，传播的梯度会是$w_{l,h}(t)$(递归网络的权重)的倍数；因此在递归层数较深的时候，梯度会消失掉(当$|w_{l,h}*y^{'}_{l}|<1$)或者爆炸(当$|w_{l,h}*y^{'}_{l}|>1$)。由于RNN存在“梯度消失和爆炸”问题，RNN的“记忆”只能是很短期的，并不具备长期的记忆。

B) 为了解决梯度消失和爆炸”问题，一种更为巧妙的递归网络结构LSTM(Long Short-Term Memory)cite{5}被设计了出来。在LSTM中，RNN中递归的隐层单元被一个存储单元(LSTMUnit)所替代，每个存储单元由一个输入门(InputGate)，一个输出门(OutputGate)和一个长期的内部的通过遗忘门(ForgetGate)更新的内部状态(Cell)相关联,如图\ref{fig:LSTM}。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\linewidth]{"fig/LSTM"}
	\caption{LSTM(Long Short-Term Memory)示意图}
	\label{fig:LSTM}
\end{figure}

内部状态Cell可以可以理解为模型存储的长期记忆：每进行一次递归迭代的时候，Cell会通过遗忘门遗忘掉部分记忆，同时通过输入门决定当前输入有多少有效信息是需要被记住的，从而得到新的记忆。最终的输出通过当前新的记忆得到，由输出们决定新的记忆中哪些是当前需要的。在LSTM的反向传播过程中，不同于RNN中梯度是一个连乘的形式(由于链式法则)，可以转化成一个连加的形式，因此有效的避免了梯度的消失和爆炸，从而具备一定的长期记忆的能力。在基础的LSTM基础上，学者们提出了多种LSTM的变种，比如\cite{10}、GRU\cite{11}和 Clockwork RNN\cite{12},他们在计算性能上会有较大的差别，然而效果基本没太大差距\cite{13,14}。一个基本的LSTM更新公式如下：
\begin{eqnarray}
i_t &=& \sigma(W_{hi} * h_{t-1} + W_{xi} * x_t + b_i)
\\
f_t &=& \sigma(W_{hf} * h_{t-1} + W_{xf} * x_t + b_f)
\\
o_t &=& \sigma(W_{ho} * h_{t-1} + W_{xo} * x_to+ b_o)
\\
g_t &=& tanh(W_{hg} * h_{t-1} + W_{xg} * x_t + b_g)
\\
c_t &=& (f_t .* c_{t-1}) + (i_t .* g_t)
\\
h_t &=& o_t .* tanh(c_t)
\end{eqnarray}

C) 递归神经网络外主要能有效的处理“序列”相关的问题，因此被大量的用在NLP的问题中。除了RNN外，也有一些其他的模型有类似功能，例如神经图灵机(Neural Turing Machine, NTM\cite{7})和记忆神经网络(Memory Networks,MenNN\cite{8,9}),他们在不同场景下会比RNN“记住”更久远的信息，从而得到更好的效果。神经图灵机的主要思想是使用一个$M*N$的矩阵取存储一份长期记忆（这与LSTM是类似的，只是LSTM维护的是一个向量），该矩阵和一个神经网络共同进行学习和预测。记忆矩阵会通过选择性的读和写来进行迭代更新，同时由于每部分都是可微的，因此可以通过梯度下降法进行训练。NTM的基本工作原理如下图：

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{"fig/NTM"}
	\caption{Neural Turing Machine（NTM）示意图}
	\label{fig:NTM}
\end{figure}

记忆神经网络\cite{8}主要用在长期记忆的推断，网络会从一个长文本中自动的将重要的信息编码后记录下来。最后产出的模型能回答关于长文本的任何问题——根据问题从记忆中寻找相关内容，然后产生答案。一个经典的MemNN的预测过程由简单的4步组成：
\begin{itemize}
	\item[-] 将输入$x$编码成一个隐向量$I(x)$。
	\item[-] 更新记忆$m_i$，$m_i=G(m_i, I(x), m)$。即通过当前的隐向量，当前记忆，整体记忆，去更新记忆中的一块内容。
	\item[-] 通过当天的记忆内容和输入决定输出向量。$o = O(I(x),m)$
	\item[-] 最后将输出向量解析成最终的回答。$r = R(o)$
\end{itemize}
然而MemNN的一个问题在于并不能End-to-End的去学习，同时NTM和MemNN并没有关注输入的顺序信息。

3，在个性化搜索中，最为重要的是怎么去理解和认识一个淘宝的用户。除了用户的一些基本画像信息，我们拥有最为关键的、与其他平台不同的数据是用户在淘宝上的行为。由于用户在淘宝上的行为天然是一个长期的行为序列，因此很自然考虑使用RNN等序列模型取进行处理。一个最基础的模型结构如图\ref{fig:MULTI-LSTM}。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{"fig/MULTI-LSTM"}
	\caption{淘宝序列匹配模型示意图}
	\label{fig:MULTI-LSTM}
\end{figure}

从图中我们可以看到网络主要由3大部分组成，分别是：1，首先得到用户的行为序列；2，将用户的行为序列经过一个带记忆的网络编码成隐向量H；3，将H通过多目标网络训练不同的目标。那么下面我们将从这三方面详细说明淘宝搜索中的序列匹配模型。

a)首先是用户序列部分。我们使用用户有过行为的商品序列作为用户的表示，每一个商品被embedding到一个128维的向量中，这个向量可以从word2vec的方法进行无监督学习得到，也可以从一个长期fine tune的深度神经网络得到。在获得商品表示的算法中，一个商品embedding前的编码主要包括商品ID、店铺、类目、价格信息。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"fig/Item_embedding"}
	\caption{商品embedding}
	\label{fig:Item_embedding}
\end{figure}

商品的embedding部分是在训练训练网络前提前训练好的，我们并没有将其放到训练序列的网络中，主要是因为ID特征十分稀疏，在一个LSTM的网络中，数据可能并不支持训练这么大规模的特征维度，从而影响模型的整体效果。然而这样的问题是，预训练得到的商品只包含了基本的商品特征，可能并不最适合当前的序列匹配网络。因此受transfer learning的启发，
向量经过embedding的商品向量并不是直接作为特征输入到LSTM或者MemNN中，而是根据商品的行为类型（点击、成交、收藏、加购）和来源经过不同的卷积核生成一个新的128维向量，然后输入到序列网络。这样既对用户的行为进行了区分，可以学习得到不同行为的重要性；同时对预训练得到的商品向量往新的目标上调整。

b)在得到用户的商品行为序列后，我们需要使用一个序列或者记忆模型，将序列编码成一个通用的用户状态H。这里我们对比了LSTM和End-to-end memory network\cite{9}。LSTM在上文中已经有过一些介绍，而一个End-to-end memory network与经典的memNN的区别在于它可以通过一个整体的网络去学习，基本的网络结构如图\ref{fig:end2end}。它首先将输入序列中的每一项同时映射成2个向量$m_i$和$c_i$，分别表示“输入记忆”和“输出记忆”。“输入记忆”决定序列中每一项的重要性$p_i$，$p_i$和和$c_i$相乘求和得到输出向量$o$。输出向量$o$和用户向量决定最终的答案$a$。LSTM使用一个记忆单元$Cell$去记忆历史信息；而End-to-end memory network正着重于将原始序列压缩，并自动挖掘序列中元素的重要性。我们使用两种方法在大量数据上进行了实验对比，从AUC来看LSTM会略优于End-to-end memory network，但是End-to-end memory network在计算速度上会远优于LSTM。
\begin{eqnarray}
c_i &=& \sigma(W_{c} * x_{i}
\\
m_i &=& \sigma(W_{m} * x_{i}
\\
p_i &=& Softmax(u^T m_i)
\\
o &=& \sum_{i}p_i c_i
\\
a &=& Softmax(W(o+u))
\end{eqnarray}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{"fig/end2end"}
	\caption{End-to-end memory network}
	\label{fig:end2end}
\end{figure}

c)在得到的用户的隐向量后，进行匹配是容易的，只需要一个不用太深的DNN网络对用户向量H和商品向量放到一起进行预测即可。但是为了学到更加鲁棒的网络结构，我们使用multi-task的相关技术\cite{16}建立了多个辅助目标共同学习。因为multi-task learning不是本章重点，因此不再详细介绍，结果部分会有相关对比结果。

4，结果展示

\section{排序学习}
	@元涵，@凌运， @龙楚
	\subsection{个性化体验}
	在搜索中有一些体验问题，如年龄，可以转换成某种约束，如$f(user\_age,auc\_age)\geq0$。在ltr模型中加入这种约束，变成带约束的优化问题，可以同时实现搜索体验和指标的最大化。
	

\section{展示学习}
	个性化短标题：@苏哲，@仁重 

\section{模型参数优化} 
	@公达

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{\protect\numberline{}{\hspace{-1.5em}参考文献}}
\markboth{参考文献}{参考文献}
\bibitem{1} 淘宝搜索全链路有效行为量化模型(UBM\&UCM), http://www.atatech.org/articles/38550
\bibitem{2} User Browsing Model的实现与应用, http://www.atatech.org/articles/23111
\bibitem{3} 搜索个性化介绍, http://www.atatech.org/articles/48548
\bibitem{4} Mikolov, T., Karafi´at, M., Burget, L., Cernock`y, J., Khudanpur, S.: Recurrent neural network based language model. J. Interspeech. 1045–1048 (2010)
\bibitem{5}  Hochreiter, S., Schmidhuber J.: Long short-term memory. J. Neural Computation. 9(8), 1735–1780 (1997)
\bibitem{6} Learning Long-Term Dependencies with Gradient Descent is Difficult
\bibitem{7} Graves, A., Wayne, G., Danihelka, I.: Neural Turing Machine. arXiv preprint:1410.5401v2 (2014)
\bibitem{8} Weston, J., Chopra, S., Bordes, A.: Memory Networks. C. International Conference on Learning Representations. arXiv:1410.3916 (2015)
\bibitem{9} Sukhbaatar, S., Szlam, A., Weston, J., Fergus, R.: End-To-End Memory Networks. J. Advances in Neural Information Processing Systems. 28, 2440–2448 (2015)
\bibitem{10} Gers, Felix A and Schmidhuber, J: Recurrent Nets that Time and Count. J. in IJCNN 2000
\bibitem{11} Cho, Kyunghyun and Van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshuai: Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. arXiv preprint arXiv:1406.1078 (2014)
\bibitem{12} Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen: A Clockwork RNN. J. arXiv preprint arXiv:1402.3511 (2014)
\bibitem{13} Zaremba, Wojciech: An Empirical Exploration of Recurrent Network Architectures. in LMLR 2015
\bibitem{14} Greff, Klaus and Srivastava, Rupesh K and Koutnik, Jan and Steunebrink, Bas R and Schmidhuber, J: A search space odyssey. IEEE transactions on neural networks and learning systems (2016)
\bibitem{15} Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial: Cross-stitch networks for multi-task learning. CVPR (2016)
\end{thebibliography}

 
