
\chapter{个性化搜索背后的核心技术}
\thispagestyle{empty}

\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
\noindent\shadowbox{
\begin{tcolorbox}[arc=0mm,colback=lightblue,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~强化学习．} \\ 

\end{tcolorbox}}
\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt} 

综述性的东西，@三桐，@公达

\section{匹配学习}
	
\subsection{一阶人货匹配模型} 
	@公达 u2i，u2s，u2b 

\subsection{高阶人货匹配模型} 
	@公达 u2i2i，u2s2i，u2b2i 

\subsection{深度匹配模型} 

\subsection{序列匹配模型} 
1，前面三个章节我们递进的描述了用户与单个商品之间的匹配方式和模型。然而，上述方法均假设用户的购物行为之间是独立的——并不存在依赖、相关或序列关系。

举例来说，一个用户$U_1$2天内依次购买了以下商品：烤箱、面粉、奶油；另一个用户$U_2$半年内依次购买了孕妇衣、尿布和奶瓶。我们先考虑用户$U_1$，我们可以从他购买了烤箱和面粉2种商品推断他很可能想要做蛋糕（而这从每个单一买的商品都是很难推断的），因此也许需要奶油；再考虑用户$U_2$，我们可以从她依次购买了孕妇衣和尿布推断她很可能怀孕过并且已经生了小宝宝（从某一件来推荐会比较勉强），因此马上会需要奶瓶等婴儿用品。

从上面例子我们可以看出，用户的购物行为之间往往是存在高阶依赖关系的，即仅用户购买了一个商品集合$\{A, B, C\}$后，才会购买商品$D$；同时，用户的购物行为也会存在序列关系，即用户购买C，仅会在他依次购买了商品A和B之后。在这2种关系下，我们前3节使用的模型会很难捕捉这类规律。因此我们需要一种模型，能整体的考虑用户的行为历史（而不是将其行为拆分成一个一个的单独分析），进而推断他接下来的需求。

下面我们会首先介绍几种经典的序列模型以及带有记忆功能的模型，然后会详细介绍在淘宝搜索中，我们怎样使用这类模型做到用户与商品之间的序列匹配。

2，在机器学习的任务环境中，我们有大量的场景都是需要做一个序列预测和带有记忆的推断的。例如在query自动补全的场景下，我们需要根据用户输入的文字或者词序列来预测用户下一个最可能会输入的词语；又例如有这样一个问题，需要让机器在阅读了一整篇文章后，回答若干关于这个文章的问题。这类问题和场景下都需要模型具有一定的记忆能力，能在获取新信息的同时，记住部分老的信息。

A) 最常用而有效的方式是使用一个递归神经网络模型(RNN \cite{4,5})。正如其名字描述的，递归神经网络在隐层结构上存在一个循环，即当前隐层的输入是上一个隐层的输出以及当前的输入2项一起。由于每个隐层的信息都能递归的输入到下一个隐层中，因此会具有一定的记忆能力。如图\ref{fig:RNN}，我们将RNN按时间序列“打开”，可以看到前一时刻的隐层$S_{t-1}$和当前输入$X_t$会共同影响当前的隐层$S_{t}$。

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"fig/RNN"}
	\caption{递归神经网络(RNN)示意图}
	\label{fig:RNN}
\end{figure}	

然而RNN存在的最大问题是“梯度消失和爆炸”问题\cite{6}。这是因为在神经网络进行反向传播(backpropagation)的时候，传播的梯度会是$w_{l,h}(t)$(递归网络的权重)的倍数；因此在递归层数较深的时候，梯度会消失掉(当$|w_{l,h}*y^{'}_{l}|<1$)或者爆炸(当$|w_{l,h}*y^{'}_{l}|>1$)。由于RNN存在“梯度消失和爆炸”问题，RNN的“记忆”只能是很短期的，并不具备长期的记忆。

B) 为了解决梯度消失和爆炸”问题，一种更为巧妙的递归网络结构LSTM(Long Short-Term Memory)cite{5}被设计了出来。在LSTM中，RNN中递归的隐层单元被一个存储单元(LSTMUnit)所替代，每个存储单元由一个输入门(InputGate)，一个输出门(OutputGate)和一个长期的内部的通过遗忘门(ForgetGate)更新的内部状态(Cell)相关联,如图\ref{fig:LSTM}。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\linewidth]{"fig/LSTM"}
	\caption{LSTM(Long Short-Term Memory)示意图}
	\label{fig:LSTM}
\end{figure}

内部状态Cell可以可以理解为模型存储的长期记忆：每进行一次递归迭代的时候，Cell会通过遗忘门遗忘掉部分记忆，同时通过输入门决定当前输入有多少有效信息是需要被记住的，从而得到新的记忆。最终的输出通过当前新的记忆得到，由输出们决定新的记忆中哪些是当前需要的。在LSTM的反向传播过程中，不同于RNN中梯度是一个连乘的形式(由于链式法则)，可以转化成一个连加的形式，因此有效的避免了梯度的消失和爆炸，从而具备一定的长期记忆的能力。在基础的LSTM基础上，学者们提出了多种LSTM的变种，比如\cite{10}、GRU\cite{11}和 Clockwork RNN\cite{12},他们在计算性能上会有较大的差别，然而效果基本没太大差距\cite{13,14}。

C) 递归神经网络外主要能有效的处理“序列”相关的问题，因此被大量的用在NLP的问题中。除了RNN外，也有一些其他的模型有类似功能，例如神经图灵机(Neural Turing Machine, NTM\cite{7})和记忆神经网络(Memory Networks,MenNN\cite{8,9}),他们在不同场景下会比RNN“记住”更久远的信息，从而得到更好的效果。神经图灵机的主要思想是使用一个$M*N$的矩阵取存储一份长期记忆（这与LSTM是类似的，只是LSTM维护的是一个向量），该矩阵和一个神经网络共同

3，序列模型具体在搜索场景的应用

4，结果展示
	
\section{排序学习}
	@元涵，@凌运， @龙楚
\subsection{}

\section{展示学习}
	个性化短标题：@苏哲，@仁重 

\section{模型参数优化} 
	@公达

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{\protect\numberline{}{\hspace{-1.5em}参考文献}}
\markboth{参考文献}{参考文献}
\bibitem{1} 淘宝搜索全链路有效行为量化模型(UBM\&UCM), http://www.atatech.org/articles/38550
\bibitem{2} User Browsing Model的实现与应用, http://www.atatech.org/articles/23111
\bibitem{3} 搜索个性化介绍, http://www.atatech.org/articles/48548
\bibitem{4} Mikolov, T., Karafi´at, M., Burget, L., Cernock`y, J., Khudanpur, S.: Recurrent neural network based language model. J. Interspeech. 1045–1048 (2010)
\bibitem{5}  Hochreiter, S., Schmidhuber J.: Long short-term memory. J. Neural Computation. 9(8), 1735–1780 (1997)
\bibitem{6} Learning Long-Term Dependencies with Gradient Descent is Difficult
\bibitem{7} Graves, A., Wayne, G., Danihelka, I.: Neural Turing Machine. arXiv preprint:1410.5401v2 (2014)
\bibitem{8} Weston, J., Chopra, S., Bordes, A.: Memory Networks. C. International Conference on Learning Representations. arXiv:1410.3916 (2015)
\bibitem{9} Sukhbaatar, S., Szlam, A., Weston, J., Fergus, R.: End-To-End Memory Networks. J. Advances in Neural Information Processing Systems. 28, 2440–2448 (2015)
\bibitem{10} Gers, Felix A and Schmidhuber, J: Recurrent Nets that Time and Count. J. in IJCNN 2000
\bibitem{11} Cho, Kyunghyun and Van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshuai: Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. arXiv preprint arXiv:1406.1078 (2014)
\bibitem{12} Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen: A Clockwork RNN. J. arXiv preprint arXiv:1402.3511 (2014)
\bibitem{13} Zaremba, Wojciech: An Empirical Exploration of Recurrent Network Architectures. in LMLR 2015
\bibitem{14} Greff, Klaus and Srivastava, Rupesh K and Koutnik, Jan and Steunebrink, Bas R and Schmidhuber, J: A search space odyssey. IEEE transactions on neural networks and learning systems (2016)
\end{thebibliography}

 
