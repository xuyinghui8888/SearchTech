
\chapter{强化学习研究}
\thispagestyle{empty}

\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
\noindent\shadowbox{
\begin{tcolorbox}[arc=0mm,colback=lightblue,colframe=darkblue,title=学习目标与要求]
\kai\textcolor{darkblue}{1.~~强化学习．} \\ 

\end{tcolorbox}}
\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}


\section{强化学习} 

超大规模状态，动作空间下，值表形式的值函数所需要的存贮空间以及稀疏性导致无法
完成有效的计算；通常采用值函数估计来解决；讲值函数进行参数化


排序权重的欧式空间就是agent的动作空间，状态空间也表征成连续的数值空间；
因此问题是在两个连续的数值空间下学习最优的映射关系；策略逼近方法是通常
解决连续状态和连续策略空间的有效方法，其主要思想和值函数估计类似，即用参数化的函数对策略
进行表达，通过优化参数来完成策略的学习，参数的策略函数通常称为actor；
我们采用确定性策略梯度算法来进行排序的实时调控优化；

actor的输出是一个确定性的策略（即某个动作), 因为在实际的onlien服务时候，我们
无法去执行在连续空间下的online 的argmax操作；因为如果是stochastic的策略
解空间下，我们得到的动作的概率分布，而不是确定的策略，这样，要求我们去在策略
空间下执行argmax的搜索；在连续动作空间下，确定性策略函数可以大幅提升策略
改进的效率；我们采用的actor以转台的特征作为输入，以最终生效的排序权重分
作为输出。这样我们一共需要调控m个维度的排序权重，对于任给状态s，actor对应
的输出表示为： 




\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{\protect\numberline{}{\hspace{-1.5em}参考文献}}
\markboth{参考文献}{参考文献}
\bibitem{1} C. Burges, T. Shaked, etc.., Learning to rank 
using gradient descent. In Proceedings of the 22nd international 
conference on machine learning, ACM
\end{thebibliography}

 
